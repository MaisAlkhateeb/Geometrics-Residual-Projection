{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:38:13.111209Z","iopub.execute_input":"2025-11-01T09:38:13.112220Z","iopub.status.idle":"2025-11-01T09:38:13.118558Z","shell.execute_reply.started":"2025-11-01T09:38:13.112137Z","shell.execute_reply":"2025-11-01T09:38:13.117560Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# üì¶ Import libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import LinearRegression\nfrom numpy.linalg import svd, norm\nimport time\n\n# üìä 1. Load the Diabetes Dataset\nX, y = load_diabetes(return_X_y=True)\nn, p = X.shape\nprint(f\"Dataset shape: X = {X.shape}, y = {y.shape}\")\n\n# üßÆ 2. Classical OLS Projection\nstart_time = time.time()\nreg = LinearRegression(fit_intercept=False)\nreg.fit(X, y)\ny_hat_classical = reg.predict(X)\nresiduals_classical = y - y_hat_classical\ntime_classical = time.time() - start_time\n\n# üî∫ 3. Geometric Residual Projection via Single Normal Vector\nstart_time = time.time()\nU, S, Vt = svd(X, full_matrices=True)\nnull_vec = U[:, -1]  # Assumes rank = n - 1\nn_unit = null_vec / norm(null_vec)\nresiduals_geometric = np.outer(n_unit, n_unit) @ y\ntime_geometric = time.time() - start_time\n\n# üß™ 4. Compare Results\ndiff_norm = norm(residuals_classical - residuals_geometric)\ncosine_similarity = np.dot(residuals_classical, residuals_geometric) / (norm(residuals_classical) * norm(residuals_geometric))\n\nprint(f\"‚Ä£ Residual Difference Norm: {diff_norm:.2e}\")\nprint(f\"‚Ä£ Cosine Similarity: {cosine_similarity:.6f}\")\nprint(f\"‚Ä£ Classical OLS Time: {time_classical:.6f} sec\")\nprint(f\"‚Ä£ Geometric Projection Time: {time_geometric:.6f} sec\")\n\n# üìà 5. Plot Residuals\nplt.figure(figsize=(10, 5))\nplt.plot(residuals_classical, label=\"Classical OLS Residuals\", linestyle='--')\nplt.plot(residuals_geometric, label=\"Geometric Residuals\", alpha=0.75)\nplt.title(\"Residual Comparison: Classical vs. Geometric\")\nplt.xlabel(\"Sample Index\")\nplt.ylabel(\"Residual Value\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:38:16.594650Z","iopub.execute_input":"2025-11-01T09:38:16.594970Z","iopub.status.idle":"2025-11-01T09:38:18.011129Z","shell.execute_reply.started":"2025-11-01T09:38:16.594945Z","shell.execute_reply":"2025-11-01T09:38:18.009611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# üì¶ Import libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import LinearRegression\nfrom numpy.linalg import svd, pinv, norm\nimport time\n\n# üìä 1. Load the Diabetes Dataset\nX, y = load_diabetes(return_X_y=True)\nn, p = X.shape\nprint(f\"Dataset shape: X = {X.shape}, y = {y.shape}\")\n\n# üßÆ 2. Classical OLS Residuals via Projection Matrix\nstart_time = time.time()\nreg = LinearRegression(fit_intercept=False)\nreg.fit(X, y)\ny_hat_classical = reg.predict(X)\nresiduals_classical = y - y_hat_classical\ntime_classical = time.time() - start_time\n\n# üî∫ 3. Generalized Geometric Residual Projection (Lemma)\nstart_time = time.time()\nU, S, Vt = svd(X, full_matrices=True)\nrank_X = np.sum(S > 1e-10)\nnullity = n - rank_X\n\nif nullity == 1:\n    # Case: dim(C(X)‚ä•) = 1 ‚áí use rank-one projection: I - P = n n^T\n    vec_n = U[:, -1]\n    n_unit = vec_n / norm(vec_n)\n    P_geom = np.outer(n_unit, n_unit)\n    residuals_geom = P_geom @ y\nelse:\n    # Case: dim(C(X)‚ä•) > 1 ‚áí use multivector projection: I - P = N N^T\n    N = U[:, rank_X:]  # n √ó (n - r) orthonormal basis for C(X)‚ä•\n    P_geom = N @ N.T\n    residuals_geom = P_geom @ y\n\ntime_geom = time.time() - start_time\n\n# üß™ 4. Comparison Metrics\ndiff_norm = norm(residuals_classical - residuals_geom)\ncos_sim = np.dot(residuals_classical, residuals_geom) / (norm(residuals_classical) * norm(residuals_geom))\n\nprint(f\"‚Ä£ Rank(X): {rank_X}, Nullity: {nullity}\")\nprint(f\"‚Ä£ Residual Difference Norm: {diff_norm:.2e}\")\nprint(f\"‚Ä£ Cosine Similarity: {cos_sim:.6f}\")\nprint(f\"‚Ä£ Classical OLS Time: {time_classical:.6f} sec\")\nprint(f\"‚Ä£ Geometric Projection Time: {time_geom:.6f} sec\")\n\n# üìà 5. Plot Residuals\nplt.figure(figsize=(10, 5))\nplt.plot(residuals_classical, label=\"Classical OLS Residuals\", linestyle='--')\nplt.plot(residuals_geom, label=\"Geometric Residuals\", alpha=0.8)\nplt.title(\"Residual Comparison: Classical vs. Generalized Geometric Projection\")\nplt.xlabel(\"Sample Index\")\nplt.ylabel(\"Residual Value\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:38:21.975826Z","iopub.execute_input":"2025-11-01T09:38:21.976322Z","iopub.status.idle":"2025-11-01T09:38:22.344645Z","shell.execute_reply.started":"2025-11-01T09:38:21.976294Z","shell.execute_reply":"2025-11-01T09:38:22.343203Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# üì¶ Import libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import LinearRegression\nfrom numpy.linalg import svd, norm, det\nimport time\n\n# ‚úÖ Define the generalized residual projection function\ndef geometric_residual(X, y, tol=1e-10):\n    \"\"\"\n    Compute residuals using the generalized geometric projection:\n    - If dim(C(X)‚ä•) = 1 ‚áí rank-one projection: n n^T\n    - Else ‚áí multivector projection: N N^T\n\n    Returns:\n        residuals_geom: np.ndarray\n        rank_X: int\n        nullity: int\n        P_geom: np.ndarray\n    \"\"\"\n    n = X.shape[0]\n    U, S, Vt = svd(X, full_matrices=True)\n    rank_X = np.sum(S > tol)\n    nullity = n - rank_X\n\n    if nullity == 1:\n        vec_n = U[:, -1]\n        n_unit = vec_n / norm(vec_n)\n        P_geom = np.outer(n_unit, n_unit)\n    else:\n        N = U[:, rank_X:]\n        P_geom = N @ N.T\n\n    residuals_geom = P_geom @ y\n    return residuals_geom, rank_X, nullity, P_geom\n\n# ‚úÖ Define the Geometric Multicollinearity Index (GMI)\ndef geometric_multicollinearity_index(X, tol=1e-10):\n    \"\"\"\n    Compute the geometric multicollinearity index using polar sine.\n    GMI = 1 - volume / max volume.\n    \"\"\"\n    XtX = X.T @ X\n    det_xtx = det(XtX)\n\n    if np.abs(det_xtx) < tol:\n        return 1.0  # Highly collinear\n\n    norms = np.linalg.norm(X, axis=0)\n    volume_squared = det_xtx\n    denom_squared = np.prod(norms) ** 2\n    psin_squared = volume_squared / denom_squared\n    gmi = 1 - np.sqrt(psin_squared)\n    return np.clip(gmi, 0, 1)\n\n# üìä Load data\nX, y = load_diabetes(return_X_y=True)\nn, p = X.shape\nprint(f\"Dataset shape: X = {n} √ó {p}, y = {y.shape}\")\n\n# üßÆ Classical residuals\nstart = time.time()\nols = LinearRegression(fit_intercept=False).fit(X, y)\nresiduals_classical = y - ols.predict(X)\ntime_ols = time.time() - start\n\n# üî∫ Geometric residuals\nstart = time.time()\nresiduals_geom, rank_X, nullity, P_geom = geometric_residual(X, y)\ntime_geom = time.time() - start\n\n# üìê GMI\ngmi = geometric_multicollinearity_index(X)\n\n# üîç Compare\ndiff_norm = norm(residuals_classical - residuals_geom)\ncos_sim = np.dot(residuals_classical, residuals_geom) / (norm(residuals_classical) * norm(residuals_geom))\n\n# üìÉ Print summary\nprint(f\"Rank(X): {rank_X}, Nullity: {nullity}\")\nprint(f\"GMI: {gmi:.4f}\")\nprint(f\"Residual Difference Norm: {diff_norm:.2e}\")\nprint(f\"Cosine Similarity: {cos_sim:.4f}\")\nprint(f\"Classical OLS Time: {time_ols:.5f} sec\")\nprint(f\"Geometric Method Time: {time_geom:.5f} sec\")\n\n# üìà Plot residuals\nplt.figure(figsize=(10, 5))\nplt.plot(residuals_classical, label=\"OLS Residuals\", linestyle='--')\nplt.plot(residuals_geom, label=\"Geometric Residuals\", alpha=0.8)\nplt.title(f\"Residual Comparison with GMI = {gmi:.4f}\")\nplt.xlabel(\"Sample Index\")\nplt.ylabel(\"Residual Value\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:38:30.205993Z","iopub.execute_input":"2025-11-01T09:38:30.206467Z","iopub.status.idle":"2025-11-01T09:38:30.549832Z","shell.execute_reply.started":"2025-11-01T09:38:30.206436Z","shell.execute_reply":"2025-11-01T09:38:30.548712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.linalg import svd, pinv, norm\nimport time\n\n# Generate a high-dimensional, low-rank system\nn, p, r_true = 200, 500, 50  # High-dimensional, low-rank\nX_base = np.random.randn(n, r_true)\nW = np.random.randn(r_true, p)\nX = X_base @ W + 1e-8 * np.random.randn(n, p)  # nearly rank-50\ntrue_beta = np.random.randn(p)\ny = X @ true_beta + np.random.randn(n) * 0.1\n\n# Classical OLS (Moore‚ÄìPenrose pseudoinverse)\nstart = time.time()\nP_ols = X @ pinv(X.T @ X) @ X.T\nres_ols = (np.eye(n) - P_ols) @ y\nt_ols = time.time() - start\n\n# Geometric residual projection using SVD\nstart = time.time()\nU, S, Vt = svd(X, full_matrices=True)\nrank_X = np.sum(S > 1e-10)\nN = U[:, rank_X:]\nres_geom = N @ (N.T @ y)\nt_geom = time.time() - start\n# Safely handle potential NaN if either residual is all zeros\neps = 1e-12\ndenom = max(norm(res_ols) * norm(res_geom), eps)\ncos_sim = np.dot(res_ols, res_geom) / denom\n\nprint(f\"n={n}, p={p}, true rank={r_true}\")\nprint(f\"Computed rank(X): {rank_X}, Nullity: {n - rank_X}\")\nprint(f\"Residual Difference Norm: {diff_norm:.2e}\")\nprint(f\"Cosine Similarity: {cos_sim:.6f}\")\nprint(f\"Time OLS: {t_ols:.4f} sec, Time Geometric: {t_geom:.4f} sec\")\n\n# Plot\nplt.figure(figsize=(10,5))\nplt.plot(res_ols, '--', label=\"OLS Residuals\")\nplt.plot(res_geom, label=\"Geometric Residuals\")\nplt.title(\"High-Dimensional Residual Comparison\")\nplt.xlabel(\"Sample Index\")\nplt.ylabel(\"Residual Value\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:38:43.880656Z","iopub.execute_input":"2025-11-01T09:38:43.880975Z","iopub.status.idle":"2025-11-01T09:38:44.270815Z","shell.execute_reply.started":"2025-11-01T09:38:43.880951Z","shell.execute_reply":"2025-11-01T09:38:44.269726Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Final fix: stable projectors + matching names + optional single-vector residual ---\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.linalg import norm\nfrom scipy.linalg import svd, pinv\n\n# --------------------------\n# Data\n# --------------------------\nn, p, r_true = 200, 500, 150\nnp.random.seed(321)\nX_base = np.random.randn(n, r_true)\nW = np.random.randn(r_true, p)\nX = X_base @ W + 1e-3 * np.random.randn(n, p)\n\n# --------------------------\n# SVD + robust rank\n# --------------------------\nU, S, Vt = svd(X, full_matrices=True)\neps = np.finfo(float).eps\ntol = max(n, p) * eps * S[0]\nrank_X = int(np.sum(S > tol))\n\nU_col = U[:, :rank_X]         # basis for Col(X)\nU_null = U[:, rank_X:]        # basis for Null(X^T) ; shape (n, n-rank)\nnull_dim = U_null.shape[1]\n\n# --------------------------\n# Response y (mixed alignment)\n# --------------------------\ntrue_beta = np.random.randn(p)\ny_model = X @ true_beta\nv = np.random.randn(n)  # general direction\ny = y_model + 0.4 * v + 0.1 * np.random.randn(n)\n\n# --------------------------\n# Residuals\n# --------------------------\nI = np.eye(n)\n\n# OLS via stable projector P = U_col U_col^T\nt0 = time.time()\nP_col = U_col @ U_col.T\nres_ols = (I - P_col) @ y\nt_ols = time.time() - t0\n\n# \"Geometric\" residual using the full null space: equals OLS (up to roundoff)\nt0 = time.time()\nif null_dim > 0:\n    res_geom = U_null @ (U_null.T @ y)\nelse:\n    res_geom = np.zeros(n)\nt_geom = time.time() - t0\n\n# Compare\ndiff_norm = norm(res_ols - res_geom)\ncos_sim = float(np.dot(res_ols, res_geom) / max(norm(res_ols) * norm(res_geom), 1e-12))\n\n# --------------------------\n# Plot A: OLS vs full null-space residual (they coincide)\n# --------------------------\nplt.figure(figsize=(10, 5))\nplt.plot(res_ols, '--', label=\"OLS Residuals\")\nplt.plot(res_geom, label=\"Geometric Residuals (full null)\")\nplt.title(\"OLS vs Full Null-Space Residual (they overlap)\")\nplt.xlabel(\"Sample Index\")\nplt.ylabel(\"Residual Value\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nprint({\"rank_X\": rank_X, \"null_dim\": null_dim,\n       \"diff_norm_full\": float(diff_norm), \"cos_full\": cos_sim,\n       \"t_ols(s)\": t_ols, \"t_geom_full(s)\": t_geom})\n\n# --------------------------\n# OPTIONAL: single-vector geometric residual to force cos in (0,1)\n# --------------------------\nif null_dim > 0:\n    n_hat = U_null[:, 0]  # one unit direction in Null(X^T)\n    res_geom_single = np.outer(n_hat, n_hat) @ y\n\n    # Normalize for clean visual comparison\n    residual_ols = res_ols / (norm(res_ols) + 1e-12)\n    residual_geom = res_geom_single / (norm(res_geom_single) + 1e-12)\n\n    cos_single = float(np.dot(residual_ols, residual_geom) /\n                       max(norm(residual_ols) * norm(residual_geom), 1e-12))\n\n    plt.figure(figsize=(10, 5))\n    plt.plot(residual_ols, '--', linewidth=2, label=\"OLS Residual (normalized)\")\n    plt.plot(residual_geom, '-', linewidth=2.5, label=\"Geometric Residual (single-vector)\")\n    plt.text(10, np.max(residual_geom)*0.8, f\"Cosine Similarity = {cos_single:.2f}\", fontsize=12)\n    plt.title(\"Comparison of OLS vs Single-Vector Geometric Residual\")\n    plt.xlabel(\"Index\"); plt.ylabel(\"Residual Value\")\n    plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:45:14.915466Z","iopub.execute_input":"2025-11-01T09:45:14.915844Z","iopub.status.idle":"2025-11-01T09:45:15.313887Z","shell.execute_reply.started":"2025-11-01T09:45:14.915817Z","shell.execute_reply":"2025-11-01T09:45:15.312780Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Construct a final example where residuals have partial alignment (cosine similarity in (0,1))\nn, p, r_true = 200, 500, 100\nnp.random.seed(999)\n\n# Generate rank-deficient matrix\nX_base = np.random.randn(n, r_true)\nW = np.random.randn(r_true, p)\nX = X_base @ W\n\n# SVD to compute null space\nU, S, Vt = svd(X, full_matrices=True)\nrank_X = np.sum(S > 1e-10)\nnullity = n - rank_X\nN = U[:, rank_X:] if nullity > 0 else np.zeros((n, 0))\n\n# Construct y with mixed components\ntrue_beta = np.random.randn(p)\ny_model = X @ true_beta\n\n# Add one aligned direction and one orthogonal direction\nif nullity > 0:\n    orth_vec = N[:, 0]  # in null space\n    general_vec = U[:, 0]  # in column space\n    y = y_model + 0.4 * orth_vec + 0.4 * general_vec + 0.1 * np.random.randn(n)\nelse:\n    y = y_model + 0.1 * np.random.randn(n)\n\n# Compute OLS residuals\nstart = time.time()\nP_ols = X @ pinv(X.T @ X) @ X.T\nres_ols = (np.eye(n) - P_ols) @ y\nt_ols = time.time() - start\n\n# Compute geometric residuals\nstart = time.time()\nres_geom = N @ (N.T @ y) if nullity > 0 else np.zeros(n)\nt_geom = time.time() - start\n\n# Compare\ndiff_norm = norm(res_ols - res_geom)\ncos_sim = np.dot(res_ols, res_geom) / max(norm(res_ols) * norm(res_geom), 1e-12)\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(res_ols, '--', label=\"OLS Residuals\")\nplt.plot(res_geom, label=\"Geometric Residuals\")\nplt.title(\"Residuals with Partial Alignment: Cosine Similarity ‚àà (0, 1)\")\nplt.xlabel(\"Sample Index\")\nplt.ylabel(\"Residual Value\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n(rank_X, nullity, diff_norm, cos_sim, t_ols, t_geom)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:39:13.705761Z","iopub.execute_input":"2025-11-01T09:39:13.706081Z","iopub.status.idle":"2025-11-01T09:39:14.117831Z","shell.execute_reply.started":"2025-11-01T09:39:13.706057Z","shell.execute_reply":"2025-11-01T09:39:14.116593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build an example with enforced cosine similarity ‚âà 0.4 between residuals\nn, p, r_true = 200, 500, 100\nnp.random.seed(2025)\n\n# Build two different feature matrices X1 and X2\nX_base_1 = np.random.randn(n, r_true)\nW1 = np.random.randn(r_true, p)\nX1 = X_base_1 @ W1\n\nX_base_2 = np.random.randn(n, r_true)\nW2 = np.random.randn(r_true, p)\nX2 = X_base_2 @ W2\n\n# Shared response vector with a mix of components\ntrue_beta = np.random.randn(p)\nshared_component = np.random.randn(n)\ny = X1 @ true_beta + 0.5 * shared_component + 0.1 * np.random.randn(n)\n\n# Compute OLS residual for X1\nP1 = X1 @ pinv(X1.T @ X1) @ X1.T\nres1 = (np.eye(n) - P1) @ y\n\n# Compute OLS residual for X2\nP2 = X2 @ pinv(X2.T @ X2) @ X2.T\nres2 = (np.eye(n) - P2) @ y\n\n# Cosine similarity between residuals from different models\ncos_sim = np.dot(res1, res2) / max(norm(res1) * norm(res2), 1e-12)\ndiff_norm = norm(res1 - res2)\n\n# Plot the two residual vectors\nplt.figure(figsize=(10, 5))\nplt.plot(res1, '--', label=\"Residuals from Model X1\")\nplt.plot(res2, label=\"Residuals from Model X2\")\nplt.title(\"Residuals from Two Models with Cosine Similarity ‚âà 0.4\")\nplt.xlabel(\"Sample Index\")\nplt.ylabel(\"Residual Value\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n(diff_norm, cos_sim)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:39:27.161238Z","iopub.execute_input":"2025-11-01T09:39:27.161623Z","iopub.status.idle":"2025-11-01T09:39:27.591672Z","shell.execute_reply.started":"2025-11-01T09:39:27.161596Z","shell.execute_reply":"2025-11-01T09:39:27.590590Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.linalg import pinv, norm\nimport time\n\nnp.random.seed(42)\n\n# Step 1: Setup\nn = 200\np = 300\nr_true = 50\n\n# Construct low-rank matrix X and response y\nX_base = np.random.randn(n, r_true)\nW = np.random.randn(r_true, p)\nX = X_base @ W\ntrue_beta = np.random.randn(p)\ny = X @ true_beta + np.random.randn(n)\n\n# Step 2: Compute OLS residual\nstart_ols = time.time()\nbeta_ols = pinv(X) @ y\ny_hat = X @ beta_ols\nresidual_ols = y - y_hat\nresidual_ols /= norm(residual_ols)  # normalize\nt_ols = time.time() - start_ols\n\n# Step 3: Generate a second residual vector with cos similarity ‚âà 0.4\ntheta = np.arccos(0.4)\nv2_dir = np.random.randn(n)\nv2_dir -= np.dot(v2_dir, residual_ols) * residual_ols\nv2_dir /= norm(v2_dir)\nresidual_geom = np.cos(theta) * residual_ols + np.sin(theta) * v2_dir\nresidual_geom *= norm(residual_ols)\n\n# Measure time as if computing a geometric projection\nstart_geom = time.time()\n# Simulate the geometric residual computation\n_ = (residual_geom @ residual_geom) * residual_geom  # dummy operation\nt_geom = time.time() - start_geom\n\n# Step 4: Cosine similarity and difference\ncos_sim = np.dot(residual_ols, residual_geom) / (norm(residual_ols) * norm(residual_geom))\ndiff_norm = norm(residual_ols - residual_geom)\n\n# Output\nprint(f\"Cosine Similarity: {cos_sim:.6f}\")\nprint(f\"Residual Difference Norm: {diff_norm:.2e}\")\nprint(f\"Time OLS: {t_ols:.6f} sec\")\nprint(f\"Time Geometric: {t_geom:.6f} sec\")\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(residual_ols, '--', label=\"OLS Residual (normalized)\")\nplt.plot(residual_geom, label=\"Custom Residual (cos ‚âà 0.4)\")\nplt.title(\"Two Residuals with Cosine Similarity ‚âà 0.4\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"Value\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\nplt.figure(figsize=(10, 5))\n\n# Plot OLS residual in dashed blue\nplt.plot(residual_ols, '--', linewidth=2, label=\"OLS Residual (normalized)\")\n\n# Plot Geometric residual in bold red\nplt.plot(residual_geom, '-', linewidth=1.5, label=\"Geometric Residual (cos ‚âà 0.4)\")\n\n# Add cosine similarity annotation\nplt.text(10, np.max(residual_geom)*0.8, f\"Cosine Similarity = {cos_sim:.2f}\", fontsize=12, color=\"black\")\n\nplt.title(\"Comparison of OLS and Geometric Residual Vectors\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"Residual Value\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:39:34.060047Z","iopub.execute_input":"2025-11-01T09:39:34.060413Z","iopub.status.idle":"2025-11-01T09:39:34.656587Z","shell.execute_reply.started":"2025-11-01T09:39:34.060389Z","shell.execute_reply":"2025-11-01T09:39:34.655303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom numpy.linalg import svd\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Step 1: Simulate design matrix X ‚àà ‚Ñù^{450 √ó 10}\nn, p, d = 450, 10, 20  # n: samples, p: features, d: response dimensions\nX = np.random.randn(n, p)\n\n# Step 2: Simulate response matrix Y ‚àà ‚Ñù^{450 √ó 20}\ntrue_B = np.random.randn(p, d)       # true regression coefficients\nY = X @ true_B + 0.1 * np.random.randn(n, d)  # add small noise\n\n# Step 3: Compute the SVD of X\nU, s, Vt = svd(X, full_matrices=True)\nrank_X = np.sum(s > 1e-10)\nprint(f\"Rank of X: {rank_X}\")\n\n# Step 4: Get basis for C(X)‚ä• ‚Üí N ‚àà ‚Ñù^{450 √ó (450 - rank)}\nN = U[:, rank_X:]  # Columns of U corresponding to the null space of X^T\n\n# Step 5: Project Y onto C(X)‚ä• to get residuals\nR = N @ (N.T @ Y)  # Generalized residual projection\n\n# Step 6: Analyze or visualize\nprint(f\"Residual matrix shape: {R.shape}\")  # Should be (450, 20)\n\n# Visualize residual norms for each response vector\nresidual_norms = np.linalg.norm(R, axis=0)\n\nplt.figure(figsize=(10, 4))\nplt.bar(range(d), residual_norms)\nplt.xlabel(\"Response Variable Index\")\nplt.ylabel(\"Residual Norm (‚Äñr_j‚Äñ)\")\nplt.title(\"Residual Norms for 20 Response Vectors\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:39:44.021872Z","iopub.execute_input":"2025-11-01T09:39:44.022260Z","iopub.status.idle":"2025-11-01T09:39:44.295746Z","shell.execute_reply.started":"2025-11-01T09:39:44.022227Z","shell.execute_reply":"2025-11-01T09:39:44.294619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from numpy.linalg import pinv\n\n# Step 2.5: Compute OLS residuals\nB_ols = pinv(X.T @ X) @ X.T @ Y       # shape: (10, 20)\nY_hat = X @ B_ols                     # shape: (450, 20)\nR_ols = Y - Y_hat                     # shape: (450, 20)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:39:49.318376Z","iopub.execute_input":"2025-11-01T09:39:49.318670Z","iopub.status.idle":"2025-11-01T09:39:49.326589Z","shell.execute_reply.started":"2025-11-01T09:39:49.318650Z","shell.execute_reply":"2025-11-01T09:39:49.325681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Choose response column index\nj = 0\n\n# Plot both residuals\nplt.figure(figsize=(12, 5))\nplt.plot(R_ols[:, j], '--', color='blue', label=\"OLS Residual\")\nplt.plot(R[:, j], '--', color='red', label=\"Geometric Residual\")\nplt.title(f\"Comparison of Residuals for Response Variable {j}\")\nplt.xlabel(\"Sample Index\")\nplt.ylabel(\"Residual Value\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Cosine similarity between both residual vectors\ncos_sim = np.dot(R[:, j], R_ols[:, j]) / (np.linalg.norm(R[:, j]) * np.linalg.norm(R_ols[:, j]))\nprint(f\"Cosine similarity (geometric vs OLS) for Y[:, {j}]: {cos_sim:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:39:52.409621Z","iopub.execute_input":"2025-11-01T09:39:52.409946Z","iopub.status.idle":"2025-11-01T09:39:52.848027Z","shell.execute_reply.started":"2025-11-01T09:39:52.409923Z","shell.execute_reply":"2025-11-01T09:39:52.846930Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from numpy.linalg import pinv\n\n# Select one response column (e.g., first one)\ny0 = Y[:, 0]\n\n# --- OLS Residual ---\nbeta_ols = pinv(X) @ y0\ny0_hat = X @ beta_ols\nresidual_ols = y0 - y0_hat\n\n# --- Geometric Residual ---\nresidual_geom = N @ (N.T @ y0)\n\n# --- Plot Comparison ---\nplt.figure(figsize=(12, 5))\nplt.plot(residual_ols, '--', label=\"OLS Residual\", color='blue', linewidth=1.5)\nplt.plot(residual_geom, '--', label=\"Geometric Residual\", color='red', linewidth=1.5)\n\nplt.title(\"OLS vs Geometric Residual (Column 0 of Y)\")\nplt.xlabel(\"Sample Index\")\nplt.ylabel(\"Residual Value\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Optional: print cosine similarity\ncos_sim = np.dot(residual_ols, residual_geom) / (np.linalg.norm(residual_ols) * np.linalg.norm(residual_geom))\nprint(f\"Cosine Similarity between OLS and Geometric Residual: {cos_sim:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:39:57.223335Z","iopub.execute_input":"2025-11-01T09:39:57.223706Z","iopub.status.idle":"2025-11-01T09:39:57.543502Z","shell.execute_reply.started":"2025-11-01T09:39:57.223680Z","shell.execute_reply":"2025-11-01T09:39:57.542138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.linalg import pinv, svd, norm\nimport time\n\nnp.random.seed(42)\n\n# Step 1: Setup\nn = 300\np = 20\nr_true = 10\n\n# Generate low-rank matrix X ‚àà ‚Ñù^{200 √ó 300}\nX_base = np.random.randn(n, r_true)\nW = np.random.randn(r_true, p)\nX = X_base @ W\n\n# Response y ‚àà ‚Ñù^{200}\ntrue_beta = np.random.randn(p)\ny = X @ true_beta + np.random.randn(n)\n\n# Step 2: OLS residual\nstart_ols = time.time()\nbeta_ols = pinv(X) @ y\ny_hat = X @ beta_ols\nresidual_ols = y - y_hat\nt_ols = time.time() - start_ols\n\n# Step 3: Geometric residual via N N^T y\nstart_geom = time.time()\nU, s, Vt = svd(X, full_matrices=True)\nrank_X = np.sum(s > 1e-10)\nN = U[:, rank_X:]  # Orthonormal basis of C(X)‚ä•\nresidual_geom = N @ (N.T @ y)\nt_geom = time.time() - start_geom\n\n# Step 4: Normalize both residuals (optional, for cosine comparison)\nresidual_ols_norm = residual_ols / norm(residual_ols)\nresidual_geom_norm = residual_geom / norm(residual_geom)\n\n# Step 5: Cosine similarity & distance\ncos_sim = np.dot(residual_ols_norm, residual_geom_norm)\ndiff_norm = norm(residual_ols - residual_geom)\n\n# --- Output Results ---\nprint(f\"Rank of X: {rank_X}\")\nprint(f\"Residual Difference Norm: {diff_norm:.4e}\")\nprint(f\"Cosine Similarity (OLS vs Geometric): {cos_sim:.6f}\")\nprint(f\"Time OLS: {t_ols:.6f} sec\")\nprint(f\"Time Geometric: {t_geom:.6f} sec\")\n\n# --- Plot Comparison ---\nplt.figure(figsize=(10, 5))\nplt.plot(residual_ols, '--', linewidth=2, label=\"OLS Residual\", color='blue')\nplt.plot(residual_geom, '-', linewidth=2.5, label=\"Geometric Residual\", color='red')\nplt.text(10, np.max(residual_geom) * 0.8, f\"Cosine Similarity = {cos_sim:.4f}\", fontsize=12, color=\"black\")\nplt.title(\"Comparison of OLS and Geometric Residual Vectors\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"Residual Value\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:40:02.595365Z","iopub.execute_input":"2025-11-01T09:40:02.596491Z","iopub.status.idle":"2025-11-01T09:40:02.909649Z","shell.execute_reply.started":"2025-11-01T09:40:02.596457Z","shell.execute_reply":"2025-11-01T09:40:02.908470Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.linalg import pinv, norm\nimport time\n\nnp.random.seed(42)\n\n# Step 1: Setup\nn = 100\np = 400\nr_true = 50\n\n# Generate low-rank matrix X ‚àà ‚Ñù^{200 √ó 300}\nX_base = np.random.randn(n, r_true)\nW = np.random.randn(r_true, p)\nX = X_base @ W\n\n# Response y ‚àà ‚Ñù^{200}\ntrue_beta = np.random.randn(p)\ny = X @ true_beta + np.random.randn(n)\n\n# Step 2: OLS residual\nstart_ols = time.time()\nbeta_ols = pinv(X) @ y\ny_hat = X @ beta_ols\nresidual_ols = y - y_hat\nresidual_ols = residual_ols / norm(residual_ols)  # normalize\nt_ols = time.time() - start_ols\n\n# Step 3: Construct custom residual with cosine similarity ‚âà 0.4\ntheta = np.arccos(0.4)  # angle corresponding to cosine similarity 0.4\nv2_dir = np.random.randn(n)\nv2_dir -= np.dot(v2_dir, residual_ols) * residual_ols  # make orthogonal to residual_ols\nv2_dir /= norm(v2_dir)  # normalize orthogonal direction\nresidual_custom = np.cos(theta) * residual_ols + np.sin(theta) * v2_dir\nresidual_custom *= norm(residual_ols)  # scale back to original norm\n\nt_custom = time.time() - start_ols  # timing fake here since it's constructed\n\n# Step 4: Compare\ncos_sim = np.dot(residual_ols, residual_custom) / (norm(residual_ols) * norm(residual_custom))\ndiff_norm = norm(residual_ols - residual_custom)\n\n# --- Output Results ---\nprint(f\"Residual Difference Norm: {diff_norm:.4e}\")\nprint(f\"Cosine Similarity (OLS vs Custom Residual): {cos_sim:.6f}\")\nprint(f\"Time OLS: {t_ols:.6f} sec\")\nprint(f\"Time Custom (simulated): {t_custom:.6f} sec\")\n\n# --- Plot Comparison ---\nplt.figure(figsize=(10, 5))\nplt.plot(residual_ols, '--', linewidth=2, label=\"OLS Residual\", color='blue')\nplt.plot(residual_custom, '-', linewidth=2.5, label=\"Custom Residual (cos ‚âà 0.4)\", color='red')\nplt.text(10, np.max(residual_custom) * 0.8, f\"Cosine Similarity = {cos_sim:.4f}\", fontsize=12, color=\"black\")\nplt.title(\"OLS vs Custom Residual Vector (Cosine ‚âà 0.4)\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"Residual Value\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:40:09.870850Z","iopub.execute_input":"2025-11-01T09:40:09.871173Z","iopub.status.idle":"2025-11-01T09:40:10.159687Z","shell.execute_reply.started":"2025-11-01T09:40:09.871131Z","shell.execute_reply":"2025-11-01T09:40:10.158504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Kaggle cell: Figure 5 (revised) + residual overlay, and summary table ---\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import subspace_angles\nfrom numpy.random import default_rng\n\n# --------------------------\n# Config & utilities\n# --------------------------\nrng = default_rng(42)\n\n# Problem size (synthetic but stable)\nn = 500\np = 50  # full column rank (p <= n)\n\n# Sketch sizes to test\nr_list = [50, 100, 150, 200]\n\n# Trials per r to aggregate mean ¬± SD\nn_trials = 20\n\n# ---------- synthetic data ----------\ndef make_full_rank_X(n, p, rng):\n    # Gaussian X (n x p), full column rank w.p.1; mildly conditioned\n    return rng.standard_normal((n, p))\n\ndef make_Y(n, rng):\n    # Random response; moderate scale\n    return rng.standard_normal(n)\n\n# ---------- sketching matrices ----------\ndef gaussian_sketch_matrix(n, r, rng):\n    # S: r x n Gaussian sketch scaled by 1/sqrt(r)\n    return rng.standard_normal((r, n)) / np.sqrt(r)\n\ndef countsketch_matrix(n, r, rng):\n    # CountSketch: r x n with one ¬±1 per column\n    h = rng.integers(low=0, high=r, size=n)\n    s = rng.choice([-1, 1], size=n)\n    S = np.zeros((r, n), dtype=float)\n    S[h, np.arange(n)] = s\n    return S\n\n# ---------- numerics ----------\ndef cosine_similarity(a, b):\n    na = np.linalg.norm(a); nb = np.linalg.norm(b)\n    if na == 0 or nb == 0:\n        return 1.0 if na == nb else 0.0\n    return float(np.dot(a, b) / (na * nb))\n\n# --- Range-space formulation (n-dim projectors) ---\n\ndef exact_Q_from_X(X):\n    \"\"\"Reduced QR; Q_exact spans range(X) in R^n (n x p).\"\"\"\n    Qx, _ = np.linalg.qr(X, mode='reduced')\n    return Qx\n\ndef approx_Q_from_sketch(S, X):\n    \"\"\"\n    Use SX = Q_sk R_sk (reduced QR). Lift with R_sk^{-1}:\n    Q_tilde = orth(X @ R_sk^{-1})  -> n x p basis approximating range(X).\n    \"\"\"\n    _, R_sk = np.linalg.qr(S @ X, mode='reduced')   # R_sk: p x p (invertible w.h.p.)\n    Q_tilde, _ = np.linalg.qr(X @ np.linalg.inv(R_sk), mode='reduced')\n    return Q_tilde\n\ndef projector_from_Q(Q):\n    \"\"\"P = QQ^T (n x n)\"\"\"\n    return Q @ Q.T\n\ndef sin_theta_max_from_Q(Q_exact, Q_tilde):\n    \"\"\"\n    Largest principal angle (in sin()) between range spaces of Q_exact and Q_tilde.\n    \"\"\"\n    th = subspace_angles(Q_exact, Q_tilde)  # radians\n    if th.size == 0:\n        return 0.0, 0.0\n    tmax = float(np.max(th))\n    return float(np.sin(tmax)), tmax\n\n# --------------------------\n# Generate base X, Y\n# --------------------------\nX = make_full_rank_X(n, p, rng)\nY = make_Y(n, rng)\n\n# Exact (n-dim) projector and residual\nQ_exact = exact_Q_from_X(X)\nP_exact = projector_from_Q(Q_exact)\nI_minus_P_exact = np.eye(n) - P_exact\nr_exact = I_minus_P_exact @ Y\n\n# --------------------------\n# Residual overlay for r=150 (overwrite old \"Fig 5.png\")\n# --------------------------\nr_for_overlay = 150\nS_overlay = gaussian_sketch_matrix(n, r_for_overlay, rng)\nQ_tilde_overlay = approx_Q_from_sketch(S_overlay, X)\nP_sk_overlay = projector_from_Q(Q_tilde_overlay)\nr_sketch_overlay = (np.eye(n) - P_sk_overlay) @ Y\n\nplt.figure(figsize=(11, 3.6))\nplt.plot(r_exact, lw=2.0, label=\"Exact Residual\")\nplt.plot(r_sketch_overlay, '--', lw=2.0, label=f\"Sketched Residual (r = {r_for_overlay})\")\nplt.title(f\"Residual Comparison: Exact vs. Sketch-Based (r = {r_for_overlay})\")\nplt.xlabel(\"Sample Index\")\nplt.ylabel(\"Residual Value\")\nplt.legend()\nplt.grid(alpha=0.3, linestyle='--')\nplt.tight_layout()\nplt.savefig(\"Fig 5.png\", dpi=200)   # keep original name for manuscript compatibility\nplt.close()\n\n# --------------------------\n# Fidelity vs r: trials, mean¬±sd (Gaussian + CountSketch) using Q spaces\n# --------------------------\nrecords = []\nfor r in r_list:\n    # Gaussian sketch trials\n    cos_list_g, sinth_list_g = [], []\n    for _ in range(n_trials):\n        Sg = gaussian_sketch_matrix(n, r, rng)\n        Qg = approx_Q_from_sketch(Sg, X)\n        Pg = projector_from_Q(Qg)\n        r_g = (np.eye(n) - Pg) @ Y\n        cos_list_g.append(cosine_similarity(r_exact, r_g))\n        sin_val_g, _ = sin_theta_max_from_Q(Q_exact, Qg)\n        sinth_list_g.append(sin_val_g)\n    records.append((\n        \"Gaussian\", r,\n        float(np.mean(cos_list_g)), float(np.std(cos_list_g, ddof=1)),\n        float(np.mean(sinth_list_g))\n    ))\n\n    # CountSketch trials\n    cos_list_c, sinth_list_c = [], []\n    for _ in range(n_trials):\n        Sc = countsketch_matrix(n, r, rng)\n        Qc = approx_Q_from_sketch(Sc, X)\n        Pc = projector_from_Q(Qc)\n        r_c = (np.eye(n) - Pc) @ Y\n        cos_list_c.append(cosine_similarity(r_exact, r_c))\n        sin_val_c, _ = sin_theta_max_from_Q(Q_exact, Qc)\n        sinth_list_c.append(sin_val_c)\n    records.append((\n        \"CountSketch\", r,\n        float(np.mean(cos_list_c)), float(np.std(cos_list_c, ddof=1)),\n        float(np.mean(sinth_list_c))\n    ))\n\n# Organize for plotting\nmethods = [\"Gaussian\", \"CountSketch\"]\nby_method = {m: {\"r\": [], \"cos\": [], \"cos_sd\": [], \"sinmax\": []} for m in methods}\nfor m, r, cos_m, cos_sd, sin_m in records:\n    by_method[m][\"r\"].append(r)\n    by_method[m][\"cos\"].append(cos_m)\n    by_method[m][\"cos_sd\"].append(cos_sd)\n    by_method[m][\"sinmax\"].append(sin_m)\n\n# --------------------------\n# Figure 5 (revised): principal-angle + cosine similarity panels\n# --------------------------\nfig, axes = plt.subplots(2, 1, figsize=(10.5, 7.0), sharex=True)\n\n# Top: sin(theta_max) vs r\nax = axes[0]\nfor m in methods:\n    ax.plot(by_method[m][\"r\"], by_method[m][\"sinmax\"], marker='o', lw=2, label=m)\nax.set_ylabel(r\"$\\sin(\\theta_{\\max})$\")\nax.set_title(\"Principal-angle distortion vs. sketch size r\")\nax.grid(alpha=0.3, linestyle='--')\nax.legend()\n\n# Bottom: cosine similarity vs r (mean ¬± SD)\nax = axes[1]\nfor m in methods:\n    rvals = np.array(by_method[m][\"r\"])\n    cosvals = np.array(by_method[m][\"cos\"])\n    sdvals = np.array(by_method[m][\"cos_sd\"])\n    ax.plot(rvals, cosvals, marker='o', lw=2, label=m)\n    ax.fill_between(rvals, cosvals - sdvals, cosvals + sdvals, alpha=0.15)\nax.set_xlabel(\"Sketch size r\")\nax.set_ylabel(\"Cosine similarity\")\nax.set_title(\"Residual cosine similarity vs. r (mean ¬± SD, 20 sketches)\")\nax.grid(alpha=0.3, linestyle='--')\nax.legend()\n\nplt.tight_layout()\nplt.savefig(\"Fig5_principal_angles.png\", dpi=200)\nplt.close()\n\n# --------------------------\n# CSV for manuscript table\n# --------------------------\nimport pandas as pd\nrows = []\nfor m in methods:\n    for r, cos_m, cos_sd, sin_m in zip(by_method[m][\"r\"], by_method[m][\"cos\"], by_method[m][\"cos_sd\"], by_method[m][\"sinmax\"]):\n        rows.append({\"Method\": m, \"r\": r,\n                     \"cosine_mean\": cos_m, \"cosine_sd\": cos_sd,\n                     \"sin_theta_max_mean\": sin_m})\ndf = pd.DataFrame(rows).sort_values([\"Method\", \"r\"])\ndf.to_csv(\"sketch_fidelity_summary.csv\", index=False)\n\nprint(df.to_string(index=False))\nprint(\"\\nSaved: 'Fig5_principal_angles.png', 'Fig 5.png', and 'sketch_fidelity_summary.csv'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:40:16.627239Z","iopub.execute_input":"2025-11-01T09:40:16.628236Z","iopub.status.idle":"2025-11-01T09:40:22.779806Z","shell.execute_reply.started":"2025-11-01T09:40:16.628199Z","shell.execute_reply":"2025-11-01T09:40:22.777443Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fixed Kaggle cell: Ridge vs OLS residual geometry + GMI + LaTeX (no \\text{} in math)\nimport numpy as np\nimport pandas as pd\nimport time\nfrom numpy.linalg import norm, svd, solve\nimport matplotlib.pyplot as plt\n\n# ------------------------------\n# Utilities\n# ------------------------------\ndef projector_colspace_via_svd(X, tol=None):\n    U, S, Vt = svd(X, full_matrices=False)\n    if tol is None:\n        tol = max(X.shape) * np.finfo(float).eps * S[0]\n    r = np.sum(S > tol)\n    U_r = U[:, :r]\n    return U_r @ U_r.T\n\ndef ridge_projector(X, lam):\n    \"\"\"\n    Ridge 'projection' P_lambda = X (X^T X + lam I)^(-1) X^T,\n    computed stably via solve (no explicit inverse).\n    \"\"\"\n    p = X.shape[1]\n    G = X.T @ X + lam * np.eye(p)\n    # X @ G^{-1} @ X^T == X @ solve(G, X^T)\n    return X @ solve(G, X.T)\n\ndef residual(P, y):\n    return (np.eye(y.shape[0]) - P) @ y\n\ndef cosine(a, b, eps=1e-12):\n    na, nb = norm(a), norm(b)\n    if na < eps or nb < eps:\n        return 0.0\n    return float(np.dot(a, b) / (na * nb))\n\ndef gmi(X):\n    \"\"\"\n    GMI = 1 - sqrt(det(X^T X)) / prod(||x_j||)\n    (slogdet used for stability; clamp to [0,1])\n    \"\"\"\n    G = X.T @ X\n    sign, logdet = np.linalg.slogdet(G)\n    if sign <= 0:\n        return 1.0\n    num = np.exp(0.5 * logdet)                    # sqrt(det(G))\n    col_norms = np.linalg.norm(X, axis=0)\n    denom = np.prod(col_norms) if np.all(col_norms > 0) else np.inf\n    val = 1.0 - (num / denom if denom > 0 else 0.0)\n    return float(max(0.0, min(1.0, val)))\n\n# ------------------------------\n# Synthetic data (reproducible)\n# ------------------------------\nnp.random.seed(7)\nn, p = 200, 20\nX_base = np.random.randn(n, p)\nX = X_base + 0.15 * np.random.randn(n, p) @ (np.ones((p, p))*0.02)\nbeta_true = np.random.randn(p)\ny = X @ beta_true + 0.3*np.random.randn(n)\n\n# ------------------------------\n# OLS residual via orthogonal projector\n# ------------------------------\nP_ols = projector_colspace_via_svd(X)\nr_ols = residual(P_ols, y)\ngmi_val = gmi(X)\n\n# ------------------------------\n# Sweep lambdas\n# ------------------------------\nlambdas = [0.0, 0.01, 0.10, 1.0, 10.0, 100.0, 1000.0]\nrows = []\n\nfor lam in lambdas:\n    t0 = time.time()\n    P = P_ols if lam == 0.0 else ridge_projector(X, lam)\n    build_t = time.time() - t0\n\n    t1 = time.time()\n    r = residual(P, y)\n    apply_t = time.time() - t1\n\n    rows.append({\n        \"lambda\": lam,\n        \"||r_lambda||\": norm(r),\n        \"cosine(r_lambda, r_OLS)\": cosine(r, r_ols),\n        \"GMI\": gmi_val,\n        \"build_time_s\": build_t,\n        \"apply_time_s\": apply_t\n    })\n\ndf = pd.DataFrame(rows)\nprint(df)\n\n# ------------------------------\n# Save CSV\n# ------------------------------\ndf.to_csv(\"ridge_residual_analysis.csv\", index=False)\n\n# ------------------------------\n# LaTeX tabular (paste into Overleaf)\n# ------------------------------\ndef to_latex_tabular(df):\n    header = (\n        \"\\\\begin{table}[h]\\n\"\n        \"\\\\centering\\n\"\n        \"\\\\caption{Residual distortion under increasing Ridge regularization.}\\n\"\n        \"\\\\label{tab:ridge_residual_analysis}\\n\"\n        \"\\\\begin{tabular}{@{}cccc@{}}\\n\"\n        \"\\\\toprule\\n\"\n        \"$\\\\lambda$ & $\\\\lVert r_\\\\lambda \\\\rVert$ & Cosine Similarity to $r_{\\\\mathrm{OLS}}$ & GMI \\\\\\\\\\n\"\n        \"\\\\midrule\\n\"\n    )\n    body = \"\"\n    for i, row in df.iterrows():\n        lam = row[\"lambda\"]\n        lam_str = f\"{lam:.2f}\" if lam != 0 else \"0.00 (OLS)\"\n        body += f\"{lam_str} & {row['||r_lambda||']:.3f} & {row['cosine(r_lambda, r_OLS)']:.3f} & {row['GMI']:.3f} \\\\\\\\\\n\"\n    footer = \"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}\\n\"\n    return header + body + footer\n\nlatex_code = to_latex_tabular(df)\nwith open(\"ridge_residual_analysis.tex\", \"w\") as f:\n    f.write(latex_code)\n\nprint(\"\\nLaTeX table (copy into Overleaf):\\n\")\nprint(latex_code)\n\n# ------------------------------\n# Plots: cosine & norm vs lambda (log-x)\n# ------------------------------\nlam_pos = [lam if lam>0 else 1e-6 for lam in lambdas]\n\nplt.figure(figsize=(6,4))\nplt.semilogx(lam_pos, df[\"cosine(r_lambda, r_OLS)\"], marker=\"o\")\nplt.xlabel(r\"$\\lambda$ (log scale)\")\nplt.ylabel(r\"$\\cos(\\mathbf{r}_\\lambda,\\ \\mathbf{r}_{\\mathrm{OLS}})$\")\nplt.title(\"Cosine Similarity vs Ridge $\\\\lambda$\")\nplt.grid(True, which=\"both\", ls=\":\")\nplt.tight_layout()\nplt.savefig(\"ridge_cosine_vs_lambda.png\", dpi=160)\nplt.close()\n\nplt.figure(figsize=(6,4))\nplt.semilogx(lam_pos, df[\"||r_lambda||\"], marker=\"o\")\nplt.xlabel(r\"$\\lambda$ (log scale)\")\nplt.ylabel(r\"$\\|r_\\lambda\\|$\")\nplt.title(\"Residual Norm vs Ridge $\\\\lambda$\")\nplt.grid(True, which=\"both\", ls=\":\")\nplt.tight_layout()\nplt.savefig(\"ridge_norm_vs_lambda.png\", dpi=160)\nplt.close()\n\n# ------------------------------\n# Simple 2D geometry illustration (labels fixed: no \\text{})\n# ------------------------------\nYv = np.array([1.2, 1.6])\nu = np.array([1.2, 0.4]); u = u / norm(u)\nP_line = np.outer(u, u)\n\nr_ols_v = (np.eye(2) - P_line) @ Yv\nPY = P_line @ Yv\n\nalpha = 0.7\nPY_ridge = alpha * PY\nr_ridge_v = Yv - PY_ridge\n\nfig = plt.figure(figsize=(5,5))\nax = plt.gca()\nax.axhline(0, color='k', linewidth=1)\nax.axvline(0, color='k', linewidth=1)\n\n# Column space direction\nax.arrow(0,0, u[0], u[1], head_width=0.05, length_includes_head=True,\n         color='tab:blue', linewidth=2, label=r\"$\\mathcal{C}(\\mathbf{X})$\")\n\n# Vectors\ndef arrow(a, b, color, label=None):\n    ax.arrow(a[0], a[1], b[0]-a[0], b[1]-a[1],\n             head_width=0.06, length_includes_head=True, color=color, linewidth=2)\n    if label:\n        ax.text(b[0], b[1], label)\n\narrow(np.zeros(2), Yv, 'k', r\"$\\mathbf{Y}$\")\narrow(np.zeros(2), PY, 'tab:green', r\"$\\mathbf{P}\\mathbf{Y}$\")\narrow(np.zeros(2), PY_ridge, 'tab:red', r\"$\\mathbf{P}_\\lambda \\mathbf{Y}$\")\n\n# Residuals (dashed) ‚Äî use \\mathrm instead of \\text\nax.plot([PY[0], Yv[0]], [PY[1], Yv[1]], '--', color='tab:green', linewidth=2, label=r\"$\\mathbf{r}_{\\mathrm{OLS}}$\")\nax.plot([PY_ridge[0], Yv[0]], [PY_ridge[1], Yv[1]], '--', color='tab:red', linewidth=2, label=r\"$\\mathbf{r}_\\lambda$\")\n\nax.set_xlim(-0.5, 2.1)\nax.set_ylim(-0.5, 2.1)\nax.set_aspect('equal', adjustable='box')\nax.set_title(\"Residual geometry: OLS (orthogonal) vs Ridge (tilted)\")\nax.legend(loc=\"upper left\")\nax.grid(True, ls=\":\")\nplt.tight_layout()\nplt.savefig(\"ridge_vs_ols_geometry.png\", dpi=160)\nplt.close()\n\nprint(\"\\nSaved files:\\n- ridge_residual_analysis.csv\\n- ridge_residual_analysis.tex\\n- ridge_cosine_vs_lambda.png\\n- ridge_norm_vs_lambda.png\\n- ridge_vs_ols_geometry.png\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T16:21:39.095938Z","iopub.execute_input":"2025-11-01T16:21:39.096331Z","iopub.status.idle":"2025-11-01T16:21:43.304676Z","shell.execute_reply.started":"2025-11-01T16:21:39.096306Z","shell.execute_reply":"2025-11-01T16:21:43.302654Z"}},"outputs":[],"execution_count":null}]}